from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# 1. String Indexing
indexers = [StringIndexer(inputCol=column, outputCol=column+"_index").fit(df) for column in categorical_columns + ["MOMENT"]]

# 2. Vector Assembling
feature_columns = [clm+"_index" for clm in categorical_columns]
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")

# 3. Pipeline for transformations
pipeline = Pipeline(stages=indexers + [assembler])
df_transformed = pipeline.fit(df).transform(df)

# 4. Splitting Data
(training_data, test_data) = df_transformed.randomSplit([0.8, 0.2])

# 5. Train a Random Forest Classifier
rf_classifier = RandomForestClassifier(labelCol="MOMENT_index", featuresCol="features", numClasses=8)
model = rf_classifier.fit(training_data)

# 6. Model Evaluation
predictions = model.transform(test_data)
evaluator = MulticlassClassificationEvaluator(labelCol="MOMENT_index", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print(f"Test Accuracy = {accuracy}")

# You might also want to add other evaluation metrics here based on your requirements, like f1 score, precision, recall, etc.
